{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "python-ml-intro-procedure.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNCy0DSW43Iv+63gGtmbj0h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwaldenphd/machine-learning-intro/blob/main/python_ml_intro_procedure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started With Machine Learning in Python: Lab\n",
        "\n",
        "<a href=\"http://creativecommons.org/licenses/by-nc/4.0/\" rel=\"license\"><img style=\"border-width: 0;\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" alt=\"Creative Commons License\" /></a>\n",
        "This tutorial is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc/4.0/\" rel=\"license\">Creative Commons Attribution-NonCommercial 4.0 International License</a>.\n"
      ],
      "metadata": {
        "id": "fRqJb_BA0vwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acknowledgements\n",
        "\n",
        "This lab is based on Chapter 1 \"Introduction\" from Andreas C. MÃ¼ller and Sarah Guide, *[Introduction to Machine learning With Python: A Guide for Data Scientists](https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/)* (O'Reilly, 2017)."
      ],
      "metadata": {
        "id": "KjQ_qSwO0wsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started With `scikit-learn`\n",
        "\n",
        "1. The first step is to make sure we have all the necessary packages installed in our Python environment:\n",
        "- [`NumPy`](https://numpy.org/install/)\n",
        "- [`SciPy`](https://www.scipy.org/install.html)\n",
        "- [`matplotlib`](https://matplotlib.org/3.3.3/users/installing.html)\n",
        "- [`pandas`](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html)\n",
        "\n",
        "2. A couple of options for installing the needed packages.\n",
        "\n",
        "  * 2a. We can install at the command line using `pip`:\n",
        "    * `pip install PACKAGE NAME`\n",
        "\n",
        "  * 2b. We can install using `conda`:\n",
        "    * `conda install PACKAGE NAME`\n",
        "\n",
        "  * 2c. To install in a Jupyter notebook environment:\n",
        "```Python\n",
        "# Install a pip package in the current Jupyter kernel\n",
        "import sys\n",
        "!{sys.executable} -m pip install <PACKAGE NAME>\n",
        "```\n",
        "\n",
        "3. The links above send you directly to the package installation instructions.\n",
        "\n",
        "4. To install `scikit-learn`:\n",
        "- (using pip) `pip install -U scikit-learn`\n",
        "- (using conda) `conda install -c conda-forge scikit-learn`\n",
        "\n",
        "5. For a Jupyter notebook environment:"
      ],
      "metadata": {
        "id": "gmdcCwit0zAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install a pip package in the current Jupyter kernel\n",
        "import sys\n",
        "!{sys.executable} -m pip install --user numpy\n",
        "!{sys.executable} -m pip install --user pandas\n",
        "!{sys.executable} -m pip install --user scipy\n",
        "!{sys.executable} -m pip install --user matplotlib\n",
        "!{sys.executable} -m pip install --user sckikit-learn"
      ],
      "metadata": {
        "id": "tXaO-RAg022J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifying Iris Species\n",
        "\n",
        "6. This portion of the lab will walk through the process of building a machine leraning application and model to determine petal and sepal length and width measurements for iris flowers.\n",
        "\n",
        "<p align=\"center\"><a href=\"https://github.com/kwaldenphd/machine-learning-intro/blob/main/figures/Figure_2_Iris.png?raw=true\"><img class=\"aligncenter\" src=\"https://github.com/kwaldenphd/machine-learning-intro/blob/main/figures/Figure_2_Iris.png?raw=true\" height=\"350\" /></a></p>\n",
        "\n",
        "7. In this scenario, we have existing measurements for three iris species.\n",
        "\n",
        "8. Our goal is to build a machine learning model from the existing measurements to be able to predict the species for a new iris based on its measurements.\n",
        "\n",
        "9. Because we have labeled measurements as our input data, this is going to be a ***supervised*** machine learning problem.\n",
        "\n",
        "10. Because we want to predict class membership, this is a ***classification*** problem.\n",
        "\n",
        "11. The desired output for this model is a single data point--the flower species."
      ],
      "metadata": {
        "id": "D6sSc3y51j-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Meet the Data\n",
        "\n",
        "12. The iris data is included in `sckikit-learn`.\n",
        "\n",
        "13. We will load the data by calling the `load_iris` function."
      ],
      "metadata": {
        "id": "G-xPlI_p1lKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import function\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# load dataset\n",
        "iris_dataset = load_iris()"
      ],
      "metadata": {
        "id": "IcPnaJQ61mcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. The `iris_dataset` object is similar to a dictionary, containing keys and values.\n",
        "\n",
        "15. To see the list of key-value pairs:"
      ],
      "metadata": {
        "id": "KuicTDMm1nPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show dataset keys\n",
        "print(\"Keys of iris_dataset: \\n{}\".format(iris_dataset.keys()))"
      ],
      "metadata": {
        "id": "dosZDB9r1oCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. The value for the `DESCR` key provides a short description this dataset.\n",
        "\n",
        "17. A great place to start in making sense of what data we have."
      ],
      "metadata": {
        "id": "eeB5KTgO1pwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show description of iris dataset\n",
        "print(iris_dataset['DESCR'[:193] + \"\\n...\")"
      ],
      "metadata": {
        "id": "44kizNqp1qPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. We can also look at the value for the `target_names` key to see the three species names that are working as classes in the classification algorithm."
      ],
      "metadata": {
        "id": "V39KlgdP1rcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show target names\n",
        "print(\"Target names:\", iris_dataset['target_names'])"
      ],
      "metadata": {
        "id": "tQneWkRL1sDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. This output tells us that the three iris species we are working with are setosa, versicolor, and virginica.\n",
        "\n",
        "20. We can access the value for the `feature_names` key to learn more about each feature or data point."
      ],
      "metadata": {
        "id": "cM9cEpx21uVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show feature names\n",
        "print(\"Feature names:\\n\", iris_dataset['feature_names'])"
      ],
      "metadata": {
        "id": "w__2Wrqd1u39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. We can also see how Python has stored feature data by using `type()`."
      ],
      "metadata": {
        "id": "4tsUunHX1vfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show data type\n",
        "print(\"Type of data:\", type(iris_dataset['data']))"
      ],
      "metadata": {
        "id": "rJx1jmQN1xCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. We can see that the iris data is stored as a `NumPy` array. \n",
        "\n",
        "23. Each row in the array corresponds to a flower, and the columns are the four measurements taken for each flower.\n",
        "\n",
        "24. We can also get a sense of the scope or size of the dataset using `.shape`."
      ],
      "metadata": {
        "id": "pLFGJ2TD1zQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show shape of data\n",
        "print(\"Shape of data:\", iris_dataset['data'].shape)"
      ],
      "metadata": {
        "id": "GdEZ8uLf10Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. This output tells us we have measurements for 150 different flowers. \n",
        "\n",
        "26. These individual items are called ***samples***. \n",
        "\n",
        "27. The sample properties (in our case flower measurements) are called ***features***.\n",
        "\n",
        "28. The ***shape*** of the `NumPy` array is the number of samples multiplied by the number of features.\n",
        "\n",
        "29. We can also access feature values for the first five samples."
      ],
      "metadata": {
        "id": "x6LpxukQ11sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show first five rows of data\n",
        "print(\"First five rows of data:\\n\", iris_dataset['data'][:5])"
      ],
      "metadata": {
        "id": "gTRFzUU112dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. We can dig further into how Python has stored this data."
      ],
      "metadata": {
        "id": "lNjlWQcB13h_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show target type\n",
        "print(\"Type of target:\", type(iris_dataset['target']))"
      ],
      "metadata": {
        "id": "TlY8Tt_s14ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. The `target` array contains the species for flowers measured.\n",
        "\n",
        "32. We can also determine the shape of the `target` array."
      ],
      "metadata": {
        "id": "0L5KqsyK15iV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show shape of target\n",
        "print(\"Shape of target:\", iris_dataset['target'].shape)"
      ],
      "metadata": {
        "id": "EvY-J_08165J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Last but not least, we might want to know how species names are represent or encoded in this dataset."
      ],
      "metadata": {
        "id": "0woQ9Spp179O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show target\n",
        "print(\"Target:\\n\", iris_dataset['target'])"
      ],
      "metadata": {
        "id": "k0HtMMuc181J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Remember the `iris['target_names']` array tells us how these numbers relate to species name.\n",
        "- `0`, setosa\n",
        "- `1`, versicolor\n",
        "- `2`, virginica\n",
        "\n",
        "## Training and Testing Data\n",
        "\n",
        "35. Remember our original problem- we want to use this data to build a machine learning model that can take a new set of measurements and predict the iris species.\n",
        "\n",
        "36. The first step is to test if the model will actually work before applying it to new measurements.\n",
        "\n",
        "37. The model remembers the provided training data, so evaluating the model using the training data is not a measure of effectiveness.\n",
        "\n",
        "38. One option is to split the labeled data into two parts.\n",
        "\n",
        "39. One part will be the input data for the model, or the training data.\n",
        "\n",
        "40. The other part will be used to evaluate the model's effectivness. \n",
        "\n",
        "41. This second part is called the test data, test set, or hold-out set.\n",
        "\n",
        "42. Thankfully, `scikit-learn` contains the function `train_test_split` to facilitate the splitting process.\n",
        "\n",
        "43. The `train_test_split` function uses 75% of the labeled data as training data and reserves 25% as the test set.\n",
        "\n",
        "44. You can also customize that ratio when using the function.\n",
        "\n",
        "45. A couple of other notes on `scikit-learn` syntax before we jump back into code.\n",
        "\n",
        "46. Data is usually denoted with `X` (uppercase x), and labels are denoted by `y` (lowercase y).\n",
        "\n",
        "47. That nomenclature is based on the mathematical notation `f(X)=y`.\n",
        "\n",
        "48. `X` is the input, and `y` is the output.\n",
        "\n",
        "49. `X` is capitalized because the input data is as two-dimensional array (a matrix or table), while the `y` target is a one-dimensional array (a vector).\n",
        "\n",
        "50. So let's call `train_test_split` on our data."
      ],
      "metadata": {
        "id": "rpVlL_Xi1-0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import statements\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# train/test/split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    iris_dataset['data'], iris_dataset['target'], random_state=0)"
      ],
      "metadata": {
        "id": "ZHDWQqh71_lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "51. When called, the function shuffles the dataset using a random number generator. \n",
        "\n",
        "52. This is important because remember our original dataset was sorted by label.\n",
        "\n",
        "53. Without shuffling, we would not have data from all classes in our test set.\n",
        "\n",
        "54. We can fix the random number generator using the `random_state` parameter. \n",
        "\n",
        "55. The `train_test_split` function output is three `NumPy` arrays.\n",
        "\n",
        "56. The `X_train` array contains 75% of the dataset rows, which will serve as the training data.\n",
        "\n",
        "57. The `y_train` array contains the labels for `X_train`.\n",
        "\n",
        "58. The `X_test` array contains the remaining 25%.\n",
        "\n",
        "59. We can see the format and shape for those output arrays.\n"
      ],
      "metadata": {
        "id": "Zk9pXCx72A9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show x train shape\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "\n",
        "# show y train shape\n",
        "print(\"y_train shape:\", y_train.shape)"
      ],
      "metadata": {
        "id": "VxfJEV2-2BzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show x test shape\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "\n",
        "# show y test shape\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "vLmHLRug2CtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect the Data\n",
        "\n",
        "60. Before jumping into building the model, we need to take a look at the data to make sure our question is something that can be answered using machine learning.\n",
        "\n",
        "61. We also need to know to what degree our data will contain or be able to provide the information we need.\n",
        "\n",
        "62. Inspecting the data before building the model can also help identify outliers or other inconsistencies.\n",
        "\n",
        "63. An easy way to inspect data is through preliminary or exploratory visualization.\n",
        "\n",
        "64. We will do this for the iris data using a ***scatter plot***.\n",
        "\n",
        "65. A scatter plot will put one feature on the `x` axis, another on the `y` axis, and draw a dot for each data point.\n",
        "\n",
        "66. Plotting all of our features at once will generate a rather chaotic scatter plot.\n",
        "\n",
        "67. So we're going to use a ***pair plot*** which creates sub plots for all possible pairs of features.\n",
        "\n",
        "68. A pair plot is a reasonable solution because we are only dealing with four features, which limits the possible combinations.\n",
        "\n",
        "69. Keep in mind a pair plot breaks out the unique pairs, so it does not show all features in the same plot.\n",
        "\n",
        "70. In the plot generated by the code below, data points are colored based on the species they belong to."
      ],
      "metadata": {
        "id": "kwRJRaSJ2EYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe from data in X_train\n",
        "\n",
        "# label the columns using the strings in iris_dataset.feature_names\n",
        "iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\n",
        "\n",
        "# create a scatter matrix from the dataframe, color by y_train\n",
        "pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15),\n",
        "                           marker='o', hist_kwds={'bins': 20}, s=60,\n",
        "                           alpha=.8, cmap=mglearn.cm3)"
      ],
      "metadata": {
        "id": "DxiDurtL2F_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "71. There's a lot more we could get into with `matplotlib` syntax and `pandas`.\n",
        "\n",
        "72. To learn more: \n",
        "  * [Introduction to Matplotlib](https://github.com/kwaldenphd/matplotlib-intro)\n",
        "  * [More With Matplotlib](https://github.com/kwaldenphd/more-with-matplotlib/)\n",
        "\n",
        "73. From this pair plot, we can see the three classes separate fairly well based on sepal and petal measurements.\n",
        "\n",
        "74. This distribution suggests a machine learning model would be able to learn to distinguish species type (or class) based on these measurements.\n",
        "\n",
        "## Build Your First Model: k-Nearest Neighbors\n",
        "\n",
        "75. For our first model, we're going to use the k-nearest neighbors classification algorithm.\n",
        "\n",
        "76. To build this model, we only have to store the training dataset.\n",
        "\n",
        "77. When making predictions for new data points, the algorithm will find the point in the training data that is closest to the new point.\n",
        "\n",
        "78. The algorithm then assigns the closest point training data label to the new point.\n",
        "\n",
        "79. [Click here](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) to learn more about the k-nearest neighbors classification algorithm.\n",
        "\n",
        "80. The *k* in the algorithm name means that we can consider any number (k) of neighbors when making predictions.\n",
        "\n",
        "81. For this example, we'll only use a single neighbor.\n",
        "\n",
        "82. Let's start!"
      ],
      "metadata": {
        "id": "pWiuFESq2JvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import modules\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# create classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=1)"
      ],
      "metadata": {
        "id": "D41x2oTw2LBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "83. The `knn` object contains the algorithm we will use to build the model, using the training data.\n",
        "\n",
        "84. It also contains the algorithm that will make predictions for new data points.\n",
        "\n",
        "85. And third, it contains the information the algorithm has extracted (or learned) from the training data. \n",
        "\n",
        "86. A quick note on Python syntax-- all `scikit-learn` machine learning models are implemented in their own classes (called `Estimator` classes).\n",
        "\n",
        "87. Using a model requires first instantiating the class into an object. \n",
        "\n",
        "88. This process also lets us set the model parameters.\n",
        "\n",
        "89. Now we can use the training data to build the model.\n",
        "\n",
        "90. The `fit` method (part of the `knn` object) takes arguments that contain our training data.\n",
        "\n",
        "91. Remember our output from the `train_test_split` function output was three `NumPy` arrays.\n",
        "\n",
        "92. We will pass `X_train` and `y_train` as arguments to `fit`."
      ],
      "metadata": {
        "id": "zI6tfl002MPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit model\n",
        "knn.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "VNoBKK3j2NAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "93. The `fit` method returns the `knn` object (modified in place).\n",
        "\n",
        "94. We are seeing a string representation of our classifier.\n",
        "\n",
        "95. That representation also shows us which parameters were used to create the model.\n",
        "\n",
        "96. In this example, we're using the default parameters.\n",
        "\n",
        "97. There's a lot more we could get into with these parameters, but again we're focusing on the basics in this lab.\n",
        "\n",
        "## Making Predictions\n",
        "\n",
        "98. Now that we have a model, we can start to make predictions for unlabeled data.\n",
        "\n",
        "99. In this example, our unlabeled data would be a series of measurements.\n",
        "\n",
        "100. Let's say we have an unclassified iris with the following measurements:\n",
        "  * 5 cm sepal length\n",
        "  * 2.9 cm sepal width\n",
        "  * 1 cm petal length\n",
        "  * 0.2 cm petal width\n",
        "\n",
        "101. We could store these measurements in a `NumPy` array and calculate its shape (# samples X # of features)."
      ],
      "metadata": {
        "id": "8p5LqtFW2Opu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# store array\n",
        "X_new = np.array([[5, 2.9, 1, 0.2]])\n",
        "\n",
        "# calculate shape\n",
        "print(\"X_new.shape:\", X_new.shape)"
      ],
      "metadata": {
        "id": "HLnWir1I2Pa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "102. Okay- now we have a single flower measurements as a two-dimensional `NumPy` array.\n",
        "\n",
        "103. We have to go through the process of creating the array, becaues `scikit-learn` requires two-dimensional arrays.\n",
        "\n",
        "104. Now, we are going to call the `predict` method for the `knn` object to make prediction for `X_new`."
      ],
      "metadata": {
        "id": "CGPYbefv2Qj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# store prediction\n",
        "prediction = knn.predict(X_new)\n",
        "\n",
        "# output prediction\n",
        "print(\"Prediction:\", prediction)\n",
        "\n",
        "# show prediction\n",
        "print(\"Predicted target name:\",\n",
        "       iris_dataset['target_names'][prediction])"
      ],
      "metadata": {
        "id": "1YuEDkOY2RZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "105. Our model predicts the `X_new` iris belongs to class `0`, which corresponds to species `setosa`.\n",
        "\n",
        "## Evaluating the Model\n",
        "\n",
        "106. But we just gave the model random measurements. \n",
        "\n",
        "107. We don't actually know if this prediction is correct.\n",
        "\n",
        "108. Behold the value of the test set created earlier in this process.\n",
        "\n",
        "109. The test set gives us data that is labeled but was not used to build the model.\n",
        "\n",
        "110. We can use the test set data to make a prediction and then compare it to the known label.\n",
        "\n",
        "111. This process allows us to compute the model accuracy.\n",
        "\n",
        "112. We'll start by creating label predictions for the test set data contained in `X_test`."
      ],
      "metadata": {
        "id": "CeeOCB_i3s-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# store prediction\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# show prediction\n",
        "print(\"Test set predictions:\\n\", y_pred)"
      ],
      "metadata": {
        "id": "BwGBI12X3uEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "113. Then we can compare `y_pred` to `y_test` to calculate an accuracy score."
      ],
      "metadata": {
        "id": "mpryplbw3uzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare test scores\n",
        "print(\"Test set score: {:.2f}\".format(np.mean(y_pred == y_test)))"
      ],
      "metadata": {
        "id": "HYmUzZsF3v_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "114. We could also perform this computation using the `knn` object's `score` method."
      ],
      "metadata": {
        "id": "SLQySren3w9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate accuracy score\n",
        "print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "vgP8IyEk3x9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "115. This score tells us that, on the test set, the model made the correct species prediction 97% of the time.\n",
        "\n",
        "116. Extrapolating that accuracy score, we can expect our model to be correct 97% of the time.\n",
        "\n",
        "<p align=\"center\"><a href=\"https://github.com/kwaldenphd/machine-learning-intro/blob/main/figures/Fig_2_Snoopy.gif?raw=true\"><img class=\"aligncenter\" src=\"https://github.com/kwaldenphd/machine-learning-intro/blob/main/figures/Fig_2_Snoopy.gif?raw=true\" /></a></p>\n",
        "\n",
        "117. Congrats- you've built a machine learning model!\n",
        "\n",
        "# Next Steps\n",
        "\n",
        "118. [Click here](https://github.com/kwaldenphd/machine-learning-intro#summary) to navigate to the last section of the lab."
      ],
      "metadata": {
        "id": "XizGPT3U3zWS"
      }
    }
  ]
}